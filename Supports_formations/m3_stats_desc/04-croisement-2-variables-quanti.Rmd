# Deux variables quantitatives

On cherche à quantifier le lien entre deux variables quantitatives X et Y sur un échantillon de n individus. Par exemple : le lien entre les revenus médians et le nombre d'emploi au lieu de travail des communes.

Plusieurs coefficient permettent de quantifier ce lien :

- Coefficient de corrélation linéaire (Pearson) : $\rho_{XY}=\dfrac{\sigma_{XY}}{\sigma_{X} \cdot \sigma_{X}}$ avec $\sigma_{XY} = \dfrac{1}{n}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})$ et $\sigma_x$ l'écart-type de X.
- Coefficient de corrélation des rangs (Spearman) : c'est le coefficient précédent appliqué aux rangs $\Rightarrow$ détecte des relations non linéaires

Dans certains cas (distributions asymétriques), il faut penser à appliquer la transformation logarithmique


## Représentations graphiques
Des représentations graphiques permettent de visualiser d'éventuels effets entre variables, qu'il faudra ensuite étudier plus précisemment en les quantifier.

### Nuage de points

Un croisement de deux variables quantitatives peut se faire facilement avec un nuage simple (`geom_point()`):

```{r nuage_pts, fig.height = 3.5}
dat_pays_loire <- dat %>% filter(REG == "52")
ggplot (data = dat_pays_loire, aes (x = P14_EMPLT, y = MED13)) +
  geom_point () 
```


On peut ajouter un peu de mise en forme : 
```{r, fig.height=3.5}
g <- ggplot (data = dat_pays_loire, aes (x = P14_EMPLT, y = MED13)) +
  geom_point (colour = "blue", cex = 0.2) +
  ggtitle ("Revenu médian en fonction du nombre d'emplois") +
  ylab ("Revenu médian") +
  xlab ("Nombre d'emplois") +
  scale_x_continuous (trans = 'log10',
                      breaks = c(0,1,10,100,1000, 10000, 100000, 1000000),
                      labels = function(x) format(x, big.mark = " ", scientific = FALSE)) +
  scale_y_continuous (labels = function(x) paste (format (x, big.mark = " ",
                                                          scientific = FALSE), " €"))
g
```

Les fonctions `scale_._continous()` permettent de modifier les echelles des axes. Ici, nous avons effectué une transformation logarithmique de l'axe des X.

Pour visualiser, sans *a priori* sur sa forme, la relation entre X et Y (elle n'est pas forcément linéaire) on peut appliquer une fonction de lissage avec `geom_smooth`.

```{r, warning = F, message = F, fig.height=3.5}
g + geom_smooth (se = FALSE, color = 'green')
```

Si on veut la droite de régression, c'est aussi la fonction `geom_smooth` en ajoutant l'argument `method = "lm"`.

```{r, warning = F, message = F, fig.height=3.5}
g + geom_smooth (method = "lm", se = FALSE, color = 'red')
```


### Matrice de nuages de points

Une matrice permet d'afficher plusieurs nuages de points d'un coup. Cela est pratique quand on explore un jeu de données avec différentes variables susceptibles d'êtres corrélées entre elles. Elle s'obtient avec la fonction `ggpairs()` (du package `GGally`).

```{r,message=FALSE,warning=F, fig.height = 8}
num <- select (dat, P14_LOGVAC:PIMP13) %>%
  sample_n (10000) %>%               # On sélectionne quelques données
  log () 
ggpairs (num)                        # package "GGally nécessaire"
```  


## Coefficients de corrélation

Le **coefficient de corrélation** entre deux variables quantifie l'intensité de la liaison entre ces variables. Il varie entre +1 et -1. S'il est proche de zéro, il n'y a pas de liaison, les variables sont indépendantes l'une de l'autre. S'il est de signe positif, les variables ont tendance à varier dans le même sens, c'est-à-dire que quand l'une augmente, l'autre tend aussi à augmenter. S'il est négatif, quand l'une augmente l'autre tend à diminuer.

Le coefficient de corrélation *linéaire* classique, par défaut dans la fonction `cor()` est celui de Pearson. Attention, il est très sensible aux valeurs extrêmes et il ne rend pas bien compte des relations non linéaires.

Il entre dans la catégorie des outils paramétriques, donc attention à son interprétation quand les variables ne sont pas distribuées selon une loi normale.

$r=\dfrac{Cov(X,Y)}{\sigma_X\times\sigma_Y}$

```{r,warning=FALSE}
cor (na.omit (num))
cor (na.omit (log (num)))
```

Pour tester si vous arrivez bien à estimer à l'oeil l'intensité du lien linéaire entre deux variables, un peu d'entraînement sur [cette appli shiny](https://hekemp.shinyapps.io/Correlation_Game/) ou [celle-ci](http://shiny.science.psu.edu/apps/boast/CorrelationGuessing/).

Quand la relation n'est pas linéaire, on peut utiliser en alternative le coefficient de corrélation de Spearman. Celui-ci est basé sur les rangs. Il permet donc de mettre en évidence des relations non linéaires (pour peu qu'elles soient monotones) entre les deux variables.

```{r,warning=FALSE}
cor (na.omit (num), method = "spearman")
```

Quelques exemples de comparaison entre les corrélations de Pearson et de Spearman [ici](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/)



## Régression

Le principe de la régression est de décrire la manière dont une variable est dépendante d'une autre variable. À la différence de la corrélation, où on cherche à quantifier la manière dont 2 variables s'oriente l'une par rapport à l'autre, la régression a pour objectif d'estimer les valeurs d'une variable en connaissant les valeurs d'une autre variable. 
Le modèle usuel de régression linéaire est :
$y = \beta_0 + \beta_1 x + \varepsilon$, avec :

- $y$ variable à expliquer

- $x$ variable explicative

- $\varepsilon$ un terme d’erreur aléatoire de loi normale, d’espérance nulle et d’écart-type σ.


On veut une estimation des coefficients $\beta_0$ et $\beta_1$.
Sur R, on se sert de la fonction `lm()` pour créer un modèle ("linear model"). Cet objet permettra d'accéder à plusieurs informations de la régression.


```{r,warning=FALSE}
model <- lm(formula = NBMENFISC13 ~ P14_RP_PROP, data = dat)
summary(model)
```

On peut encore une fois visualiser la droite de régression grâce à la fonction `geom_smooth()`
```{r,warning=FALSE}
ggplot(data = dat, aes(x = P14_RP_PROP, y = NBMENFISC13)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

### Exercice

Effectuer une nouvelle régression avec 2 autres variables, puis tracer le nuage de points de ces variables, et enfin y ajouter la droite de regression.